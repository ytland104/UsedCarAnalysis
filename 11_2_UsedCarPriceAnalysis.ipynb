{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼ƒè»Šä¸¡ã‚«ã‚¿ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83%89%83%93%83h%83N%83%8B%81%5B%83U%81%5B/    ãƒ©ãƒ³ãƒ‰ã‚¯ãƒ«ãƒ¼ã‚¶ãƒ¼\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83%89%83%93%83h%83N%83%8B%81%5B%83U%81%5B%83v%83%89%83h/ã€€ã€€ãƒ©ãƒ³ãƒ‰ã‚¯ãƒ«ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ©ãƒ‰\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83A%83%8B%83t%83%40%81%5B%83h/ã€€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ¼ãƒ‰\n",
    "https://kakaku.com/kuruma/used/maker/%83%8C%83N%83T%83X/RX/ \n",
    "https://kakaku.com/kuruma/used/maker/%83%81%83%8B%83Z%83f%83X%81E%83x%83%93%83c/C%83N%83%89%83X%83X%83e%81%5B%83V%83%87%83%93%83%8F%83S%83%93/   c_class_station\n",
    "https://kakaku.com/kuruma/used/maker/%83X%83Y%83L/%83W%83%80%83j%81%5B/ ã‚·ã‚¨ãƒ©\n",
    "https://kakaku.com/kuruma/used/maker/%83%7D%83c%83%5F/%83%8D%81%5B%83h%83X%83%5E%81%5B/ ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¿ãƒ¼\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83N%83%89%83E%83%93%83A%83X%83%8A%81%5B%83g/ ã‚¯ãƒ©ã‚¦ãƒ³ã‚¢ã‚¹ãƒªãƒ¼t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "def get_clipboard_content_as_list():\n",
    "    content = pyperclip.paste()  # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã®å†…å®¹ã‚’å–å¾—\n",
    "    return content.split('\\n')   # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆã‚’è¿”ã™\n",
    "\n",
    "def copy_list_to_clipboard(lst):\n",
    "    content = str(lst)           # ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›\n",
    "    pyperclip.copy(content)      # ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "# ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã‹ã‚‰ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "clipboard_list = get_clipboard_content_as_list()\n",
    "\n",
    "# ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼\n",
    "copy_list_to_clipboard(clipboard_list)\n",
    "clipboard_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from retry import retry\n",
    "import mojimoji\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "@retry(tries=3, delay=5, backoff=2)\n",
    "def get_html(url):\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "urls = clipboard_list\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        print(url)\n",
    "        soup = get_html(url)\n",
    "        sleep_time = random.uniform(5, 35)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "        grade_boxes = soup.select('.gradeBox')\n",
    "        model_names = [model.text.strip() for model in soup.select('.modelName .modelNameInner a')]\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for idx, box in enumerate(grade_boxes):\n",
    "            headers = [th.text.strip() for th in box.select('th')]\n",
    "            rows = box.select('tr')[1:]\n",
    "            data = []\n",
    "            web_addresses = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = [td.text.strip() for td in row.select('td')]\n",
    "                \n",
    "                # å„è¡Œã‹ã‚‰webã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ã—ã¦cellsã«è¿½åŠ \n",
    "                grade_name_link = row.select_one('.gradeName a')\n",
    "                web_address = grade_name_link['href'] if grade_name_link else 'N/A'\n",
    "                cells.append(\"https://kakaku.com\" + str(web_address))\n",
    "                \n",
    "                data.append(cells)\n",
    "                \n",
    "            # åˆ—åã®ãƒªã‚¹ãƒˆã«'Web Address'ã‚’è¿½åŠ \n",
    "            headers.append('Web Address')\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            df['model'] = model_names[idx]\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        final_df['æ²è¼‰å°æ•°'] = final_df['ã‚°ãƒ¬ãƒ¼ãƒ‰å\\xa0(æ²è¼‰å°æ•°)'].str.extract(r'\\((\\d+)\\)').astype(int)\n",
    "        final_df['ã‚°ãƒ¬ãƒ¼ãƒ‰å'] = final_df['ã‚°ãƒ¬ãƒ¼ãƒ‰å\\xa0(æ²è¼‰å°æ•°)'].str.replace(r'\\(\\d+\\)', '', regex=True).str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "    def split_price(price):\n",
    "        clean_price = price.replace(\"ä¸‡å††\", \"\").replace(\",\", \"\")\n",
    "        clean_price = clean_price.replace(\"ï½\", \"~\").replace(\"ã€œ\", \"~\")\n",
    "        parts = clean_price.split(\"~\")\n",
    "        try:\n",
    "            return (float(parts[0]), float(parts[1])) if len(parts) == 2 else (float(parts[0]), float(parts[0]))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with price: {price}\\nCleaned price parts: {parts}\\nError message: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "         \n",
    "    # 1. ä¸­å¤è»Šã®æœ€å°ä¾¡æ ¼ã¨æœ€å¤§ä¾¡æ ¼ã‚’åˆ†å‰²\n",
    "    final_df['æ”¯æ‰•ç·é¡(min)'], final_df['æ”¯æ‰•ç·é¡(max)'] = zip(*final_df['æ”¯æ‰•ç·é¡'].apply(split_price))\n",
    "\n",
    "    # 2. Noneå€¤ã‚’NaNã«å¤‰æ›\n",
    "    final_df.replace({None: np.nan}, inplace=True)\n",
    "\n",
    "    # 3. åˆ—ã‚’floatå‹ã«å¤‰æ›\n",
    "    final_df['æ”¯æ‰•ç·é¡(min)'] = final_df['æ”¯æ‰•ç·é¡(min)'].astype(float)\n",
    "    final_df['æ”¯æ‰•ç·é¡(max)'] = final_df['æ”¯æ‰•ç·é¡(max)'].astype(float)\n",
    "\n",
    "    # 4. å¹³å‡ã‚’è¨ˆç®—\n",
    "    final_df['æ”¯æ‰•ç·é¡(ave)'] = ((final_df['æ”¯æ‰•ç·é¡(min)'] + final_df['æ”¯æ‰•ç·é¡(max)']) / 2).round(2)\n",
    "\n",
    "    # 5. å¹³å‡ä¾¡æ ¼ãŒNaNã®å ´åˆã¯ç©ºç™½ã«ç½®ãæ›ãˆ\n",
    "    final_df['æ”¯æ‰•ç·é¡(ave)'].fillna(\"\", inplace=True)\n",
    "\n",
    "    # 6. æœ€å°ä¾¡æ ¼ã¾ãŸã¯æœ€å¤§ä¾¡æ ¼ãŒNaNã®è¡Œã‚’å‰Šé™¤\n",
    "    final_df.dropna(subset=['æ”¯æ‰•ç·é¡(min)', 'æ”¯æ‰•ç·é¡(max)'], inplace=True)\n",
    "    \n",
    "    \n",
    "    final_df.drop(columns=['ã‚°ãƒ¬ãƒ¼ãƒ‰å\\xa0(æ²è¼‰å°æ•°)', 'æ”¯æ‰•ç·é¡','å®šå“¡', 'é§†å‹•', 'å¤‰é€Ÿ', 'ç‡ƒè²»'], inplace=True)\n",
    "    final_df['æ–°è»Šä¾¡æ ¼'] = final_df['æ–°è»Šä¾¡æ ¼'].str.replace('ä¸‡å††', '').astype(float)\n",
    "\n",
    "    final_df['æ”¯æ‰•ç·é¡(ave)'] = pd.to_numeric(final_df['æ”¯æ‰•ç·é¡(ave)'], errors='coerce')\n",
    "    final_df['æ”¯æ‰•ç·é¡ã®å‰²åˆ(%)'] = ((final_df['æ”¯æ‰•ç·é¡(ave)'] / final_df['æ–°è»Šä¾¡æ ¼']) * 100).round(2)\n",
    "\n",
    "    \n",
    "    final_df['ãƒ¢ãƒ‡ãƒ«å'] = final_df['model'].str.extract(r'^(.*?)\\s\\d+å¹´ãƒ¢ãƒ‡ãƒ«')\n",
    "    final_df['å¹´'] = final_df['model'].str.extract(r'(\\d+å¹´ãƒ¢ãƒ‡ãƒ«)')\n",
    "    # Noneå€¤ã‚’NaNã«å¤‰æ›\n",
    "    # final_df.replace({None: np.nan}, inplace=True)\n",
    "    if not final_df.empty:\n",
    "        today_date = datetime.today().strftime('%Y%m%d')\n",
    "        model_name = final_df['ãƒ¢ãƒ‡ãƒ«å'].iloc[0].replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{today_date}_{model_name}.csv\"\n",
    "        final_df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(\"DataFrame is empty!\")\n",
    "       \n",
    "    #ä¸­å¤è»Šä¾¡æ ¼ã®å¹³å‡å€¤ã¯å˜ç´”è¨ˆç®—ã§å‡ºã—ã¦ã„ã‚‹ãŒã€æœ€é »å€¤ã€ä¸­å¤®å€¤ã®ç®—å‡ºãŒå¿…è¦ã€‚ã“ã‚Œã¯ãã‚Œãã‚Œã®ä¾¡æ ¼ã‚’å–å¾—ã—ã¦åˆ†å¸ƒè¨ˆç®—ã•ã›ã‚‹ã€‚\n",
    "    #è³¼å…¥ã—ãŸå¹´æœˆæ—¥ã¨ç™ºå£²å¹´æœˆæ—¥ã‚’\n",
    "print(\"Done!!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLsã®ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"porsche.txt\", \"w\") as f:\n",
    "#     for url in clipboard_list:\n",
    "#         f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from retry import retry\n",
    "import mojimoji\n",
    "\n",
    "BASE_URL = \"https://kakaku.com\"\n",
    "\n",
    "@retry(tries=3, delay=5, backoff=2)\n",
    "def get_html(url):\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "def car_model_detection(url):\n",
    "    try:\n",
    "        p = r\"Maker=(\\d{1,2})\"\n",
    "        p1 = r\"Model=(\\d{4,5})\"\n",
    "        maker = re.findall(p, url)\n",
    "        model = re.findall(p1, url)\n",
    "        return [maker[-1], model[-1]]\n",
    "    except:\n",
    "        return [0, 0]\n",
    "\n",
    "def engine_category(pstr):\n",
    "    p = r\"(\\d{3}|\\d{1}.\\d{1})\"\n",
    "    matches = re.findall(p, pstr)\n",
    "    return matches[-1] if matches else 0\n",
    "\n",
    "def parse_car_detail_info(car_url, maker):\n",
    "    data = {}\n",
    "    soup2 = get_html(car_url)\n",
    "\n",
    "    data[\"åç§°\"] = mojimoji.zen_to_han(soup2.find(\"h3\").getText().strip())\n",
    "    data[\"Class_Category\"] = engine_category(data[\"åç§°\"])\n",
    "    data[\"Url\"] = car_url\n",
    "    data[\"Maker\"] = maker[0]\n",
    "    data[\"Model\"] = maker[1]\n",
    "\n",
    "    # data[\"ä¾¡æ ¼\"] = soup2.find(\"span\", {\"class\": \"priceTxt\"}).find(\"span\").getText().strip()\n",
    "    data[\"ç·é¡\"] = soup2.find(\"span\", {\"class\": \"-total\"}).getText().strip()\n",
    "    \n",
    "    # total = re.findall(r\"[0-9.]+\", soup2.find(\"span\", {\"class\": \"total\"}).getText().strip())\n",
    "    data[\"ä¾¡æ ¼\"] = soup2.find(\"span\", {\"class\": \"-base\"}).getText().strip()\n",
    "\n",
    "    for tr in soup2.find(\"table\", {\"class\": \"specList\"}).findAll(\"tr\"):\n",
    "        data[tr.find(\"th\").getText().strip()] = tr.find(\"td\").getText().strip()\n",
    "    for li in soup2.find(\"div\", {\"class\": \"optionArea\"}).findAll(\"li\", {\"class\": \"yes\"}):\n",
    "        data[li.getText().strip()] = 1\n",
    "    for li in soup2.find(\"div\", {\"class\": \"optionArea\"}).findAll(\"del\"):\n",
    "        data[li.getText().strip()] = 0\n",
    "    for dt, dd in zip(soup2.find_all(\"dt\"), soup2.find_all(\"dd\")):\n",
    "        data[dt.getText().strip()] = dd.getText().strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "def check_next_page(soup):\n",
    "    return 1 if soup.find(\"li\", {\"class\": \"next\"}) else 0\n",
    "\n",
    "def extract_color_from_item(item):\n",
    "    color_item = item.find(\"li\", class_=re.compile(\"color color\\d+\"))\n",
    "    if color_item:\n",
    "        color_class = [cls for cls in color_item[\"class\"] if cls.startswith(\"color\") and len(cls) > 5]\n",
    "        if color_class:\n",
    "            return color_class[0][5:]\n",
    "    return None\n",
    "\n",
    "def main(final_df):\n",
    "    all_data = []\n",
    "    originals = final_df['Web Address'].tolist()\n",
    "\n",
    "    # for url in originals[:2]:\n",
    "    for url in originals:\n",
    "            # 5ç§’ã‹ã‚‰10ç§’ã®é–“ã§ãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚é–“ã‚’ç”Ÿæˆ\n",
    "        sleep_time = random.uniform(15, 35)\n",
    "        time.sleep(sleep_time)\n",
    "        print(url)\n",
    "        for page in range(1, 37):\n",
    "            sleep_time = random.uniform(3, 10)\n",
    "            current_url = url + \"Page={}/\".format(page)\n",
    "            soup = get_html(current_url)\n",
    "\n",
    "            idx = final_df[final_df['Web Address'] == url].index[0]\n",
    "            æ–°è»Šä¾¡æ ¼ = final_df.at[idx, \"æ–°è»Šä¾¡æ ¼\"]\n",
    "            ã‚°ãƒ¬ãƒ¼ãƒ‰å = final_df.at[idx, \"ã‚°ãƒ¬ãƒ¼ãƒ‰å\"]\n",
    "            å¹´ = final_df.at[idx, \"å¹´\"]\n",
    "\n",
    "            for item in soup.findAll(\"div\", {\"class\": \"ucItemBox\"}):\n",
    "                car_url = BASE_URL + item.find(\"a\").get(\"href\")\n",
    "                detail_info = parse_car_detail_info(car_url, car_model_detection(url))\n",
    "\n",
    "                color_info = extract_color_from_item(item)\n",
    "                detail_info['color'] = color_info\n",
    "                detail_info['æ–°è»Šä¾¡æ ¼'] = æ–°è»Šä¾¡æ ¼\n",
    "                detail_info['ã‚°ãƒ¬ãƒ¼ãƒ‰å'] = ã‚°ãƒ¬ãƒ¼ãƒ‰å\n",
    "                detail_info['å¹´'] = å¹´\n",
    "                all_data.append(detail_info)\n",
    "\n",
    "            if not check_next_page(soup):\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfs = main(final_df)\n",
    "    print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ã“ã“ã¯2å›èµ°ã‚‰ã›ã‚‹ã¨ã ã‚ï¼ ä¿®å¾©å±¥æ­´ãŒå…¨ã¦â€ï¼â€ã«ãªã£ã¦ã—ã¾ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def remove_long_columns(df):\n",
    "    remove_columns = [i for i in df.columns if len(i) >= 100]\n",
    "    return df.drop(remove_columns, axis=1)\n",
    "\n",
    "def extract_year(value):\n",
    "    if isinstance(value, str) and re.search(r\"[0-9]{4}\", value):\n",
    "        return int(re.findall(r\"[0-9]{4}\", value)[0])\n",
    "    return np.nan\n",
    "\n",
    "def convert_price(value):\n",
    "    if value == \"å¿œè«‡\":\n",
    "        return np.nan\n",
    "    return float(value)\n",
    "\n",
    "def convert_distance(value):\n",
    "    value = str(value)\n",
    "    if re.search(r\"[0-9.]+\", value):\n",
    "        num = float(re.findall(r\"[0-9.]+\", value)[0])\n",
    "        return num * 10000 if \"ä¸‡\" in value else num\n",
    "    return 0\n",
    "\n",
    "def calculate_remaining_months(row):\n",
    "    current_date = datetime.now()\n",
    "    if pd.isna(row):\n",
    "        return 0\n",
    "    elif row in ['è»Šæ¤œæ•´å‚™ä»˜', 'è»Šæ¤œæ•´å‚™ãªã—']:\n",
    "        return 24 if row == 'è»Šæ¤œæ•´å‚™ä»˜' else 0\n",
    "    try:\n",
    "        year, month = map(int, row.split('/'))\n",
    "        target_date = datetime(year, month, 1)\n",
    "        diff = (target_date.year - current_date.year) * 12 + target_date.month - current_date.month\n",
    "        return diff\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def extract_category(x):\n",
    "    parts = x.split()\n",
    "    return parts[1] if len(parts) > 1 else None\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df = remove_long_columns(df)\n",
    "    \n",
    "    df[\"å¹´å¼\"] = df[\"å¹´å¼ï¼åˆåº¦ç™»éŒ²\"].apply(extract_year)\n",
    "    df[\"ä¾¡æ ¼\"] = df[\"ä¾¡æ ¼\"].apply(convert_price)\n",
    "    df[\"èµ°è¡Œè·é›¢\"] = df[\"èµ°è¡Œè·é›¢\"].apply(convert_distance)\n",
    "    df[\"color\"] = df[\"color\"].astype(float)  # è¿½åŠ \n",
    "    df['RemainingMonths'] = df['è»Šæ¤œ'].apply(calculate_remaining_months).astype(float)\n",
    "    \n",
    "    df[\"æœªä½¿ç”¨è»Š\"] = df[\"æœªä½¿ç”¨è»Š\"].apply(lambda x: 1 if x == \"â—‹\" else 0)\n",
    "    df[\"ç¦ç…™è»Š\"] = df[\"ç¦ç…™è»Š\"].apply(lambda x: 1 if x == \"â—‹\" else 0)\n",
    "    df[\"ãƒ¯ãƒ³ã‚ªãƒ¼ãƒŠãƒ¼\"] = df[\"ãƒ¯ãƒ³ã‚ªãƒ¼ãƒŠãƒ¼\"].apply(lambda x: 1 if x == \"â—‹\" else 0)\n",
    "    df[\"ä¿®å¾©æ­´\"] = df[\"ä¿®å¾©æ­´\"].apply(lambda x: 1 if x == \"ä¿®å¾©æ­´ã‚ã‚Š\" else 0)\n",
    "    \n",
    "    df = df[df[\"å¹´å¼\"] != 1991]\n",
    "    df.dropna(subset=[\"ä¾¡æ ¼\", \"å¹´å¼\"], inplace=True)\n",
    " \n",
    "    df[\"Category\"] = df.iloc[:, 0].apply(extract_category)\n",
    "    df[\"Mission\"] = df[\"ãƒŸãƒƒã‚·ãƒ§ãƒ³\"].apply(lambda x: 1 if x == \"MT\" else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = preprocess_dataframe(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\n",
    "keywords = [\n",
    "    \"åç§°\",'Url',\n",
    "    \"color\",\n",
    "    'æ–°è»Šä¾¡æ ¼',\"ä¾¡æ ¼\",'ç·é¡'\n",
    "    \"è‰²\",\n",
    "    'ã‚°ãƒ¬ãƒ¼ãƒ‰å', 'å¹´',\"å¹´å¼\",\n",
    "    \"èµ°è¡Œè·é›¢\",\n",
    "    \"åœ°åŸŸ\",\n",
    "    \"Mission\",\n",
    "    \"RemainingMonths\",\n",
    "    \"ä¿®å¾©æ­´\", 'ãƒ¯ãƒ³ã‚ªãƒ¼ãƒŠãƒ¼','ã‚¨ã‚¢ãƒ­ãƒ‘ãƒ¼ãƒ„'\n",
    "]\n",
    "\n",
    "# keywordsã«å«ã¾ã‚Œã‚‹åˆ—ã®ã¿ã‚’æ®‹ã™\n",
    "cols_to_keep = [col for col in df.columns if col in keywords]\n",
    "df_rev = df[cols_to_keep]  \n",
    "df_rev.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸­é–“ä¿å­˜ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{today_date}_{model_name}_detaildata.csv\"\n",
    "df_rev.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸­é–“ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åº¦èª­ã¿è¾¼ã‚€å ´åˆã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®scrapingå¾Œã¯ã“ã‚Œã‚’ä½¿ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# path='./20230831_ãƒ©ãƒ³ãƒ‰ã‚¯ãƒ«ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ©ãƒ‰_detaildata.csv'\n",
    "# df_rev=pd.read_csv(path)\n",
    "# # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ã‚’é™¤ãï¼‰ã‚’å–å¾—\n",
    "# filename = os.path.splitext(os.path.basename(path))[0]\n",
    "# print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸­é–“å‡¦ç†ã—ãŸdataframeã‚’CSVã«å¤‰æ›ã—ã¦ã„ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://levelup.gitconnected.com/python-libraries-for-lazy-data-scientists-c1287eb794ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„åˆ—ã®æ¬ æå€¤ã®æ•°ã‚’å–å¾—\n",
    "missing_counts = df_rev.isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# ã‚‚ã—æ¬ æå€¤ã®ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã‚ˆã‚Šå¤§ãã„å ´åˆã€æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤\n",
    "if missing_counts.sum() > 0:\n",
    "    df_rev = df_rev.dropna()\n",
    "    print(\"æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"æ¬ æå€¤ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "# å‰Šé™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’è¡¨ç¤º\n",
    "print(df_rev.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SweetVIzã«ã‚ˆã‚‹å¯è¦–åŒ–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹\n",
    "# !pip install sweetviz\n",
    "\n",
    "# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#æ—¥æœ¬èªå¯¾å¿œ\n",
    "sv.config_parser.read_string(\"[General]\\nuse_cjk_font=1\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# 3. ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ã™ã‚‹\n",
    "train_data, test_data = train_test_split(df_rev, test_size=0.2)\n",
    "\n",
    "# 4. Sweetvizã®analyzeé–¢æ•°ã¨compareé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æãƒ»æ¯”è¼ƒã™ã‚‹\n",
    "report = sv.compare([train_data, \"Training Data\"], [test_data, \"Test Data\"])\n",
    "\n",
    "# 5. ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºã™ã‚‹\n",
    "report.show_html(f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã˜ã‚…ã†ã‹ã„ãåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_rev.drop(['Url',\"åç§°\",\"åœ°åŸŸ\",\"Category\"], axis=1)\n",
    "x = df_rev.drop(['Url',\"åç§°\",\"åœ°åŸŸ\"], axis=1)\n",
    "dummies = pd.get_dummies(x, drop_first=True)\n",
    "# dummies\n",
    "print(np.isnan(dummies).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹:  ã“ã“ã§dfã®åå‰å¤‰æ›´ã—ã¦ã„ã‚‹ã®ã§ã€å®Ÿæ–½è¦ã€‚\n",
    "ä¸€èˆ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã€å¹³å‡å€¤ã‚„ä¸­å¤®å€¤ã€æœ€é »å€¤ãªã©ã‚’ç”¨ã„ã¦æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹æ³•ã¨ã—ã¦ã¯ã€ä»–ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
    "ã“ã“ã§ã¯ã€SimpleImputer ã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚ã€€ã€€---ã“ã‚Œã‚’ã™ã‚‹ã¨numpyã«ãªã£ã¦ã—ã¾ã†ã€‚\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dummies_imputed = imputer.fit_transform(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¥ ã€Œç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’è¦–è¦šåŒ–ï¼ - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ä½¿ã£ã¦ã¿ãŸï¼ã€\n",
    "\n",
    "ğŸ‰ ã“ã‚“ã«ã¡ã¯ã€çš†ã•ã‚“ï¼ä»Šæ—¥ã¯ã‚ã‚‹ã‚°ãƒ©ãƒ•ã«ã¤ã„ã¦ãŠè©±ã—ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚ã“ã®ã‚°ãƒ©ãƒ•ã€ä½•ã‹ç‰¹åˆ¥ãªã‚‚ã®ã‹ã¨ã„ã†ã¨ã€å®Ÿã¯ã€è»Šã®ä¾¡æ ¼ã‚’äºˆæ¸¬ã™ã‚‹ã®ã«å½¹ç«‹ã¤æƒ…å ±ã€ã¤ã¾ã‚Šã€ã©ã®æƒ…å ±ãŒä¸€ç•ªå¤§åˆ‡ãªã®ã‹ã‚’è¦‹ã‚‹ãŸã‚ã®ã‚‚ã®ãªã‚“ã§ã™ã€‚\n",
    "\n",
    "âœ¨ ãƒã‚¤ãƒ³ãƒˆ1ï¼šãƒãƒ¼ã®é•·ã•ãŒå¤§äº‹ï¼\n",
    "\n",
    "ã“ã®ã‚°ãƒ©ãƒ•ã®ä¸­ã§ã€ä¸€ç•ªæ°—ã‚’ã¤ã‘ã¦ã»ã—ã„ã®ã¯å„ãƒãƒ¼ã®é•·ã•ã§ã™ã€‚é•·ã„ãƒãƒ¼ã»ã©ã€ãã®æƒ…å ±ãŒä¾¡æ ¼ã®äºˆæ¸¬ã«é‡è¦ã¨ã„ã†ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€é•·ã„ãƒãƒ¼ã‚’æŒã¤é …ç›®ã¯ã€è»Šã®ä¾¡æ ¼ã‚’çŸ¥ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãªã‚“ã§ã™ï¼\n",
    "ğŸš— ãƒã‚¤ãƒ³ãƒˆ2ï¼šå…¨éƒ¨ã®æƒ…å ±ãŒå¤§äº‹ã§ã¯ãªã„ï¼\n",
    "\n",
    "ã‚°ãƒ©ãƒ•ã‚’è¦‹ã¦åˆ†ã‹ã‚‹é€šã‚Šã€ã™ã¹ã¦ã®æƒ…å ±ãŒåŒã˜ãã‚‰ã„å¤§åˆ‡ãªã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ã‚‹æƒ…å ±ã¯éå¸¸ã«ä¾¡å€¤ãŒã‚ã‚‹ä¸€æ–¹ã§ã€ä»–ã®æƒ…å ±ã¯ãã‚Œã»ã©å½±éŸ¿ã‚’æŒãŸãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ä¾‹ãˆã°ã€ã‚ã‚‹è»Šã®æ©Ÿèƒ½ã¯ä¾¡æ ¼ã«å¤§ããå½±éŸ¿ã™ã‚‹ã‘ã©ã€åˆ¥ã®å°ã•ãªæ©Ÿèƒ½ã¯ãã‚“ãªã«å½±éŸ¿ã—ãªã„ã€ã¨ã„ã†ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
    "ğŸ” ãƒã‚¤ãƒ³ãƒˆ3ï¼šæ˜ç¢ºãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒã§ãã‚‹ï¼\n",
    "\n",
    "ã“ã®ã‚°ãƒ©ãƒ•ã‚’ä½¿ãˆã°ã€ã©ã®æƒ…å ±ãŒæœ€ã‚‚ä¾¡å€¤ãŒã‚ã‚‹ã®ã‹ã€æ¬¡ã«ã©ã‚ŒãŒå¤§åˆ‡ãªã®ã‹ã€ã¨ã„ã†ã‚ˆã†ã«ã€æƒ…å ±ã®é‡è¦åº¦ã‚’ã¯ã£ãã‚Šã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€è»Šã‚’è²·ã†æ™‚ã‚„å£²ã‚‹æ™‚ã«ã€ã©ã‚“ãªãƒã‚¤ãƒ³ãƒˆã«æ³¨ç›®ã™ã‚Œã°ã„ã„ã®ã‹ã‚’çŸ¥ã‚‹æ‰‹åŠ©ã‘ã«ãªã‚Šã¾ã™ã­ï¼\n",
    "ä»¥ä¸Šã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ä½¿ã£ã¦è¦‹ãˆã¦ããŸè»Šã®ä¾¡æ ¼äºˆæ¸¬ã«å½¹ç«‹ã¤æƒ…å ±ã®é‡è¦åº¦ã«ã¤ã„ã¦ã®ãƒã‚¤ãƒ³ãƒˆã‚’ãŠä¼ãˆã—ã¾ã—ãŸï¼èˆˆå‘³ãŒæ¹§ã„ãŸæ–¹ã¯ã€æ˜¯éã€ã“ã®æŠ€è¡“ã‚’ä½¿ã£ã¦ã¿ã¦ãã ã•ã„ã­ï¼ğŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode_dataframe(dataframe):\n",
    "    result_df = dataframe.copy()\n",
    "    le_dict = {}  # å„åˆ—ã®LabelEncoderã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‹ã®åˆ—ã‚’æ¤œç´¢\n",
    "    for column in dataframe.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        result_df[column] = le.fit_transform(dataframe[column])\n",
    "        le_dict[column] = le  # é€†å¤‰æ›ã®ãŸã‚ã«LabelEncoderã‚’ä¿å­˜\n",
    "        \n",
    "    return result_df, le_dict\n",
    "\n",
    "encoded_df, le_dict = label_encode_dataframe(dummies)\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ\n",
    "'''\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# å¿…è¦ãªåˆ—ã ã‘ã‚’å–å¾—\n",
    "subset_df = df_rev.copy()\n",
    "\n",
    "# ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»\n",
    "g = sns.pairplot(data=subset_df, hue='ã‚°ãƒ¬ãƒ¼ãƒ‰å')\n",
    "\n",
    "# å„ãƒ—ãƒ­ãƒƒãƒˆã«ç›¸é–¢ä¿‚æ•°ã‚’è¿½åŠ \n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    # æ•°å€¤å‹ã®åˆ—ã®ã¿ã‚’è€ƒæ…®\n",
    "    if (np.issubdtype(subset_df.iloc[:, i].dtype, np.number) and\n",
    "            np.issubdtype(subset_df.iloc[:, j].dtype, np.number)):\n",
    "        g.axes[i, j].annotate(\n",
    "            f\"corr={subset_df.iloc[:, i].corr(subset_df.iloc[:, j]):.2f}\",\n",
    "            (0.5, 0.9), xycoords='axes fraction',\n",
    "            ha='center', va='center', fontsize=10\n",
    "        )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KFold ã¾ãŸã¯ StratifiedKFold ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã®ãƒˆãƒ¬ã‚¤ãƒ³/ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
    "å„åˆ†å‰²ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ã—ã€ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "ã™ã¹ã¦ã®åˆ†å‰²ã§å–å¾—ã—ãŸç‰¹å¾´é‡ã®é‡è¦åº¦ã®å¹³å‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "å¹³å‡ã®ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚\n",
    "ä»¥ä¸‹ã¯ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚\n",
    "'''\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆ†å‰²\n",
    "X = encoded_df.drop('ä¾¡æ ¼', axis=1)\n",
    "Y = encoded_df['ä¾¡æ ¼']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š\n",
    "'''\n",
    "k-Fold Cross Validationã¯æ‰‹å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’kå€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã—ã¦,kå€‹ã®ã†ã¡ã²ã¨ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦\n",
    "æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¾ã™ï¼ãã‚Œã‚’å…¨ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹ã‚ˆã†kå›ç¹°ã‚Šè¿”ã—ã¾ã™ï¼\n",
    "'''\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# å„åˆ†å‰²ã§ã®ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "feature_importances_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_imputed):\n",
    "    X_train, X_test = X_imputed[train_index], X_imputed[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã«é©åˆ\n",
    "    rf.fit(X_train, Y_train)\n",
    "\n",
    "    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—\n",
    "    feature_importances_list.append(rf.feature_importances_)\n",
    "\n",
    "# ã™ã¹ã¦ã®åˆ†å‰²ã§ã®ç‰¹å¾´é‡ã®é‡è¦åº¦ã®å¹³å‡ã‚’å–å¾—\n",
    "average_feature_importances = np.mean(feature_importances_list, axis=0)\n",
    "\n",
    "# é‡è¦åº¦ã®é™é †ã§ç‰¹å¾´é‡ã‚’ã‚½ãƒ¼ãƒˆ\n",
    "sorted_idx = average_feature_importances.argsort()\n",
    "\n",
    "# å¹³å‡ã®ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(X.columns[sorted_idx], average_feature_importances[sorted_idx])\n",
    "plt.xlabel(\"Average Feature Importance across CV\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"CrossValidationã§ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹å¹³å‡ç‰¹å¾´é‡ã®é‡è¦åº¦\")\n",
    "\n",
    "# ç”»åƒã¨ã—ã¦ä¿å­˜\n",
    "plt.savefig(\"feature_importances_amg_Ctouring.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #æ¨™æº–åŒ–ã®å‡¦ç†\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "# from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢\n",
    "# X = encoded_df.drop(columns='ä¾¡æ ¼')\n",
    "# y = encoded_df['ä¾¡æ ¼']\n",
    "# # æ¨™æº–åŒ–\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "# # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# # å„ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–ã—ã¦dictå‹å¤‰æ•°ã®modelsã«æ ¼ç´\n",
    "# models = {\n",
    "#     'linear': LinearRegression(),\n",
    "#     'ridge': Ridge(random_state=0),\n",
    "#     'lasso': Lasso(random_state=0)}\n",
    "\n",
    "# # æ­£è§£ç‡ã‚’æ ¼ç´ã™ã‚‹dictå‹å¤‰æ•°ã‚’åˆæœŸåŒ–\n",
    "# scores = {}\n",
    "\n",
    "# # å„ãƒ¢ãƒ‡ãƒ«ã‚’é †æ¬¡ç”Ÿæˆã—ã€æ­£è§£ç‡ã‚’ç®—å‡ºã—ã¦æ ¼ç´\n",
    "# for model_name, model in models.items():\n",
    "#     # ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ\n",
    "#     model.fit(X_train, Y_train)\n",
    "#     # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ç‡\n",
    "#     scores[(model_name, 'train')] = model.score(X_train, Y_train)\n",
    "#     # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ç‡\n",
    "#     scores[(model_name, 'test')] = model.score(X_test, Y_test)\n",
    "\n",
    "# # dictå‹ã‚’pandasã®1æ¬¡å…ƒãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "# print(pd.Series(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é‡å›å¸°äºˆæ¸¬å¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "X = encoded_df.drop(columns='ä¾¡æ ¼')\n",
    "y = encoded_df['ä¾¡æ ¼']\n",
    "\n",
    "# 2. ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. é‡å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "train_predicted = reg.predict(X_train)\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "test_predicted = reg.predict(X_test)\n",
    "\n",
    "# R^2 ã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
    "train_score = reg.score(X_train, y_train)\n",
    "test_score = reg.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R^2 score: {train_score:.2f}\")\n",
    "print(f\"Testing R^2 score: {test_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 4. æ¬ æå€¤ã®ç¢ºèªãƒ»è£œå®Œ\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# 5. äºˆæ¸¬å¼ã®è¡¨ç¤º\n",
    "print(\"Coefficients:\", reg.coef_)\n",
    "print(\"Intercept:\", reg.intercept_)\n",
    "print(\"äºˆæ¸¬å¼: \\n\")\n",
    "print(f\"ä¾¡æ ¼ = {reg.intercept_:.4f}\")\n",
    "for coef, feature in zip(reg.coef_, X.columns):\n",
    "    print(f\" + ({coef:.4f} * {feature})\")\n",
    "\n",
    "# 6. äºˆæ¸¬å€¤ã®è¨ˆç®—\n",
    "predicted = reg.predict(X)\n",
    "\n",
    "# 7. äºˆæ¸¬å€¤ã‚’dummiesã«è¿½åŠ \n",
    "encoded_df['predicted_price'] = predicted\n",
    "print(encoded_df[['ä¾¡æ ¼', 'predicted_price']])\n",
    "\n",
    "# 8. äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®ä¾¡æ ¼ã®å·®åˆ†ã‚’è¨ˆç®—\n",
    "encoded_df['difference'] = encoded_df['ä¾¡æ ¼'] - encoded_df['predicted_price']\n",
    "print(encoded_df[['ä¾¡æ ¼', 'predicted_price', 'difference']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ä¾¡æ ¼ = {reg.intercept_:.4f}\")\n",
    "for coef, feature in zip(reg.coef_, X.columns):\n",
    "    print(f\" + ({coef:.4f} * {feature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n",
    "df1['difference'] = encoded_df['difference']\n",
    "df1['predicted_price']=encoded_df['predicted_price']\n",
    "\n",
    "# df1ã®ç‰¹å®šã®åˆ—ã‚’å–å¾—\n",
    "export_data = df1[['åç§°','å¹´å¼','ã‚°ãƒ¬ãƒ¼ãƒ‰å' ,'å¹´' ,'è‰²' ,'Class_Category', 'Url', 'Mission','ä¿®å¾©æ­´'\n",
    "                   , 'difference','æ–°è»Šä¾¡æ ¼', 'ä¾¡æ ¼', 'ç·é¡','predicted_price','èµ°è¡Œè·é›¢']]\n",
    "# nanã‚’å«ã‚€è¡Œã‚’å‰Šé™¤\n",
    "export_data = export_data.dropna()\n",
    "\n",
    "\n",
    "# CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
    "filename = f\"{today_date}_{model_name}_withexpectedPrice.csv\"\n",
    "# df_rev.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "export_data.to_csv(f\"{filename}\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bokehã€€ã«ã‚ˆã‚‹å¯è¦–åŒ–ã€‚ã€€æ•£å¸ƒå›³ã‚’ä½œæˆã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SUb1 æ›´æ–°å‡¦ç†\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
