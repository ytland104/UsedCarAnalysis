{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃車両カタログデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83%89%83%93%83h%83N%83%8B%81%5B%83U%81%5B/    ランドクルーザー\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83%89%83%93%83h%83N%83%8B%81%5B%83U%81%5B%83v%83%89%83h/　　ランドクルーザープラド\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83A%83%8B%83t%83%40%81%5B%83h/　アルファード\n",
    "https://kakaku.com/kuruma/used/maker/%83%8C%83N%83T%83X/RX/ \n",
    "https://kakaku.com/kuruma/used/maker/%83%81%83%8B%83Z%83f%83X%81E%83x%83%93%83c/C%83N%83%89%83X%83X%83e%81%5B%83V%83%87%83%93%83%8F%83S%83%93/   c_class_station\n",
    "https://kakaku.com/kuruma/used/maker/%83X%83Y%83L/%83W%83%80%83j%81%5B/ シエラ\n",
    "https://kakaku.com/kuruma/used/maker/%83%7D%83c%83%5F/%83%8D%81%5B%83h%83X%83%5E%81%5B/ ロードスター\n",
    "https://kakaku.com/kuruma/used/maker/%83g%83%88%83%5E/%83N%83%89%83E%83%93%83A%83X%83%8A%81%5B%83g/ クラウンアスリーt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "def get_clipboard_content_as_list():\n",
    "    content = pyperclip.paste()  # クリップボードの内容を取得\n",
    "    return content.split('\\n')   # 改行で分割してリストを返す\n",
    "\n",
    "def copy_list_to_clipboard(lst):\n",
    "    content = str(lst)           # リストを文字列に変換\n",
    "    pyperclip.copy(content)      # クリップボードにコピー\n",
    "\n",
    "# クリップボードからリストを取得\n",
    "clipboard_list = get_clipboard_content_as_list()\n",
    "\n",
    "# リストをクリップボードにコピー\n",
    "copy_list_to_clipboard(clipboard_list)\n",
    "clipboard_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from retry import retry\n",
    "import mojimoji\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "@retry(tries=3, delay=5, backoff=2)\n",
    "def get_html(url):\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "urls = clipboard_list\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        print(url)\n",
    "        soup = get_html(url)\n",
    "        sleep_time = random.uniform(5, 35)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "        grade_boxes = soup.select('.gradeBox')\n",
    "        model_names = [model.text.strip() for model in soup.select('.modelName .modelNameInner a')]\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for idx, box in enumerate(grade_boxes):\n",
    "            headers = [th.text.strip() for th in box.select('th')]\n",
    "            rows = box.select('tr')[1:]\n",
    "            data = []\n",
    "            web_addresses = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = [td.text.strip() for td in row.select('td')]\n",
    "                \n",
    "                # 各行からwebアドレスを取得してcellsに追加\n",
    "                grade_name_link = row.select_one('.gradeName a')\n",
    "                web_address = grade_name_link['href'] if grade_name_link else 'N/A'\n",
    "                cells.append(\"https://kakaku.com\" + str(web_address))\n",
    "                \n",
    "                data.append(cells)\n",
    "                \n",
    "            # 列名のリストに'Web Address'を追加\n",
    "            headers.append('Web Address')\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            df['model'] = model_names[idx]\n",
    "            dfs.append(df)\n",
    "\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        final_df['掲載台数'] = final_df['グレード名\\xa0(掲載台数)'].str.extract(r'\\((\\d+)\\)').astype(int)\n",
    "        final_df['グレード名'] = final_df['グレード名\\xa0(掲載台数)'].str.replace(r'\\(\\d+\\)', '', regex=True).str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "    def split_price(price):\n",
    "        clean_price = price.replace(\"万円\", \"\").replace(\",\", \"\")\n",
    "        clean_price = clean_price.replace(\"～\", \"~\").replace(\"〜\", \"~\")\n",
    "        parts = clean_price.split(\"~\")\n",
    "        try:\n",
    "            return (float(parts[0]), float(parts[1])) if len(parts) == 2 else (float(parts[0]), float(parts[0]))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with price: {price}\\nCleaned price parts: {parts}\\nError message: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "         \n",
    "    # 1. 中古車の最小価格と最大価格を分割\n",
    "    final_df['支払総額(min)'], final_df['支払総額(max)'] = zip(*final_df['支払総額'].apply(split_price))\n",
    "\n",
    "    # 2. None値をNaNに変換\n",
    "    final_df.replace({None: np.nan}, inplace=True)\n",
    "\n",
    "    # 3. 列をfloat型に変換\n",
    "    final_df['支払総額(min)'] = final_df['支払総額(min)'].astype(float)\n",
    "    final_df['支払総額(max)'] = final_df['支払総額(max)'].astype(float)\n",
    "\n",
    "    # 4. 平均を計算\n",
    "    final_df['支払総額(ave)'] = ((final_df['支払総額(min)'] + final_df['支払総額(max)']) / 2).round(2)\n",
    "\n",
    "    # 5. 平均価格がNaNの場合は空白に置き換え\n",
    "    final_df['支払総額(ave)'].fillna(\"\", inplace=True)\n",
    "\n",
    "    # 6. 最小価格または最大価格がNaNの行を削除\n",
    "    final_df.dropna(subset=['支払総額(min)', '支払総額(max)'], inplace=True)\n",
    "    \n",
    "    \n",
    "    final_df.drop(columns=['グレード名\\xa0(掲載台数)', '支払総額','定員', '駆動', '変速', '燃費'], inplace=True)\n",
    "    final_df['新車価格'] = final_df['新車価格'].str.replace('万円', '').astype(float)\n",
    "\n",
    "    final_df['支払総額(ave)'] = pd.to_numeric(final_df['支払総額(ave)'], errors='coerce')\n",
    "    final_df['支払総額の割合(%)'] = ((final_df['支払総額(ave)'] / final_df['新車価格']) * 100).round(2)\n",
    "\n",
    "    \n",
    "    final_df['モデル名'] = final_df['model'].str.extract(r'^(.*?)\\s\\d+年モデル')\n",
    "    final_df['年'] = final_df['model'].str.extract(r'(\\d+年モデル)')\n",
    "    # None値をNaNに変換\n",
    "    # final_df.replace({None: np.nan}, inplace=True)\n",
    "    if not final_df.empty:\n",
    "        today_date = datetime.today().strftime('%Y%m%d')\n",
    "        model_name = final_df['モデル名'].iloc[0].replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{today_date}_{model_name}.csv\"\n",
    "        final_df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(\"DataFrame is empty!\")\n",
    "       \n",
    "    #中古車価格の平均値は単純計算で出しているが、最頻値、中央値の算出が必要。これはそれぞれの価格を取得して分布計算させる。\n",
    "    #購入した年月日と発売年月日を\n",
    "print(\"Done!!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLsのテキスト保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"porsche.txt\", \"w\") as f:\n",
    "#     for url in clipboard_list:\n",
    "#         f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from retry import retry\n",
    "import mojimoji\n",
    "\n",
    "BASE_URL = \"https://kakaku.com\"\n",
    "\n",
    "@retry(tries=3, delay=5, backoff=2)\n",
    "def get_html(url):\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "def car_model_detection(url):\n",
    "    try:\n",
    "        p = r\"Maker=(\\d{1,2})\"\n",
    "        p1 = r\"Model=(\\d{4,5})\"\n",
    "        maker = re.findall(p, url)\n",
    "        model = re.findall(p1, url)\n",
    "        return [maker[-1], model[-1]]\n",
    "    except:\n",
    "        return [0, 0]\n",
    "\n",
    "def engine_category(pstr):\n",
    "    p = r\"(\\d{3}|\\d{1}.\\d{1})\"\n",
    "    matches = re.findall(p, pstr)\n",
    "    return matches[-1] if matches else 0\n",
    "\n",
    "def parse_car_detail_info(car_url, maker):\n",
    "    data = {}\n",
    "    soup2 = get_html(car_url)\n",
    "\n",
    "    data[\"名称\"] = mojimoji.zen_to_han(soup2.find(\"h3\").getText().strip())\n",
    "    data[\"Class_Category\"] = engine_category(data[\"名称\"])\n",
    "    data[\"Url\"] = car_url\n",
    "    data[\"Maker\"] = maker[0]\n",
    "    data[\"Model\"] = maker[1]\n",
    "\n",
    "    # data[\"価格\"] = soup2.find(\"span\", {\"class\": \"priceTxt\"}).find(\"span\").getText().strip()\n",
    "    data[\"総額\"] = soup2.find(\"span\", {\"class\": \"-total\"}).getText().strip()\n",
    "    \n",
    "    # total = re.findall(r\"[0-9.]+\", soup2.find(\"span\", {\"class\": \"total\"}).getText().strip())\n",
    "    data[\"価格\"] = soup2.find(\"span\", {\"class\": \"-base\"}).getText().strip()\n",
    "\n",
    "    for tr in soup2.find(\"table\", {\"class\": \"specList\"}).findAll(\"tr\"):\n",
    "        data[tr.find(\"th\").getText().strip()] = tr.find(\"td\").getText().strip()\n",
    "    for li in soup2.find(\"div\", {\"class\": \"optionArea\"}).findAll(\"li\", {\"class\": \"yes\"}):\n",
    "        data[li.getText().strip()] = 1\n",
    "    for li in soup2.find(\"div\", {\"class\": \"optionArea\"}).findAll(\"del\"):\n",
    "        data[li.getText().strip()] = 0\n",
    "    for dt, dd in zip(soup2.find_all(\"dt\"), soup2.find_all(\"dd\")):\n",
    "        data[dt.getText().strip()] = dd.getText().strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "def check_next_page(soup):\n",
    "    return 1 if soup.find(\"li\", {\"class\": \"next\"}) else 0\n",
    "\n",
    "def extract_color_from_item(item):\n",
    "    color_item = item.find(\"li\", class_=re.compile(\"color color\\d+\"))\n",
    "    if color_item:\n",
    "        color_class = [cls for cls in color_item[\"class\"] if cls.startswith(\"color\") and len(cls) > 5]\n",
    "        if color_class:\n",
    "            return color_class[0][5:]\n",
    "    return None\n",
    "\n",
    "def main(final_df):\n",
    "    all_data = []\n",
    "    originals = final_df['Web Address'].tolist()\n",
    "\n",
    "    # for url in originals[:2]:\n",
    "    for url in originals:\n",
    "            # 5秒から10秒の間でランダムな時間を生成\n",
    "        sleep_time = random.uniform(15, 35)\n",
    "        time.sleep(sleep_time)\n",
    "        print(url)\n",
    "        for page in range(1, 37):\n",
    "            sleep_time = random.uniform(3, 10)\n",
    "            current_url = url + \"Page={}/\".format(page)\n",
    "            soup = get_html(current_url)\n",
    "\n",
    "            idx = final_df[final_df['Web Address'] == url].index[0]\n",
    "            新車価格 = final_df.at[idx, \"新車価格\"]\n",
    "            グレード名 = final_df.at[idx, \"グレード名\"]\n",
    "            年 = final_df.at[idx, \"年\"]\n",
    "\n",
    "            for item in soup.findAll(\"div\", {\"class\": \"ucItemBox\"}):\n",
    "                car_url = BASE_URL + item.find(\"a\").get(\"href\")\n",
    "                detail_info = parse_car_detail_info(car_url, car_model_detection(url))\n",
    "\n",
    "                color_info = extract_color_from_item(item)\n",
    "                detail_info['color'] = color_info\n",
    "                detail_info['新車価格'] = 新車価格\n",
    "                detail_info['グレード名'] = グレード名\n",
    "                detail_info['年'] = 年\n",
    "                all_data.append(detail_info)\n",
    "\n",
    "            if not check_next_page(soup):\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfs = main(final_df)\n",
    "    print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ここは2回走らせるとだめ！ 修復履歴が全て”０”になってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def remove_long_columns(df):\n",
    "    remove_columns = [i for i in df.columns if len(i) >= 100]\n",
    "    return df.drop(remove_columns, axis=1)\n",
    "\n",
    "def extract_year(value):\n",
    "    if isinstance(value, str) and re.search(r\"[0-9]{4}\", value):\n",
    "        return int(re.findall(r\"[0-9]{4}\", value)[0])\n",
    "    return np.nan\n",
    "\n",
    "def convert_price(value):\n",
    "    if value == \"応談\":\n",
    "        return np.nan\n",
    "    return float(value)\n",
    "\n",
    "def convert_distance(value):\n",
    "    value = str(value)\n",
    "    if re.search(r\"[0-9.]+\", value):\n",
    "        num = float(re.findall(r\"[0-9.]+\", value)[0])\n",
    "        return num * 10000 if \"万\" in value else num\n",
    "    return 0\n",
    "\n",
    "def calculate_remaining_months(row):\n",
    "    current_date = datetime.now()\n",
    "    if pd.isna(row):\n",
    "        return 0\n",
    "    elif row in ['車検整備付', '車検整備なし']:\n",
    "        return 24 if row == '車検整備付' else 0\n",
    "    try:\n",
    "        year, month = map(int, row.split('/'))\n",
    "        target_date = datetime(year, month, 1)\n",
    "        diff = (target_date.year - current_date.year) * 12 + target_date.month - current_date.month\n",
    "        return diff\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def extract_category(x):\n",
    "    parts = x.split()\n",
    "    return parts[1] if len(parts) > 1 else None\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df = remove_long_columns(df)\n",
    "    \n",
    "    df[\"年式\"] = df[\"年式／初度登録\"].apply(extract_year)\n",
    "    df[\"価格\"] = df[\"価格\"].apply(convert_price)\n",
    "    df[\"走行距離\"] = df[\"走行距離\"].apply(convert_distance)\n",
    "    df[\"color\"] = df[\"color\"].astype(float)  # 追加\n",
    "    df['RemainingMonths'] = df['車検'].apply(calculate_remaining_months).astype(float)\n",
    "    \n",
    "    df[\"未使用車\"] = df[\"未使用車\"].apply(lambda x: 1 if x == \"○\" else 0)\n",
    "    df[\"禁煙車\"] = df[\"禁煙車\"].apply(lambda x: 1 if x == \"○\" else 0)\n",
    "    df[\"ワンオーナー\"] = df[\"ワンオーナー\"].apply(lambda x: 1 if x == \"○\" else 0)\n",
    "    df[\"修復歴\"] = df[\"修復歴\"].apply(lambda x: 1 if x == \"修復歴あり\" else 0)\n",
    "    \n",
    "    df = df[df[\"年式\"] != 1991]\n",
    "    df.dropna(subset=[\"価格\", \"年式\"], inplace=True)\n",
    " \n",
    "    df[\"Category\"] = df.iloc[:, 0].apply(extract_category)\n",
    "    df[\"Mission\"] = df[\"ミッション\"].apply(lambda x: 1 if x == \"MT\" else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = preprocess_dataframe(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーワードリスト\n",
    "keywords = [\n",
    "    \"名称\",'Url',\n",
    "    \"color\",\n",
    "    '新車価格',\"価格\",'総額'\n",
    "    \"色\",\n",
    "    'グレード名', '年',\"年式\",\n",
    "    \"走行距離\",\n",
    "    \"地域\",\n",
    "    \"Mission\",\n",
    "    \"RemainingMonths\",\n",
    "    \"修復歴\", 'ワンオーナー','エアロパーツ'\n",
    "]\n",
    "\n",
    "# keywordsに含まれる列のみを残す\n",
    "cols_to_keep = [col for col in df.columns if col in keywords]\n",
    "df_rev = df[cols_to_keep]  \n",
    "df_rev.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルを中間保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{today_date}_{model_name}_detaildata.csv\"\n",
    "df_rev.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間保存したファイルを再度読み込む場合、大規模データのscraping後はこれを使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# path='./20230831_ランドクルーザープラド_detaildata.csv'\n",
    "# df_rev=pd.read_csv(path)\n",
    "# # ファイル名（拡張子を除く）を取得\n",
    "# filename = os.path.splitext(os.path.basename(path))[0]\n",
    "# print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間処理したdataframeをCSVに変換している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://levelup.gitconnected.com/python-libraries-for-lazy-data-scientists-c1287eb794ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各列の欠損値の数を取得\n",
    "missing_counts = df_rev.isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# もし欠損値のカウントが0より大きい場合、欠損値を含む行を削除\n",
    "if missing_counts.sum() > 0:\n",
    "    df_rev = df_rev.dropna()\n",
    "    print(\"欠損値を含む行を削除しました。\")\n",
    "else:\n",
    "    print(\"欠損値は存在しません。\")\n",
    "\n",
    "# 削除後のデータの形状を表示\n",
    "print(df_rev.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SweetVIzによる可視化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 必要なライブラリをインストールする\n",
    "# !pip install sweetviz\n",
    "\n",
    "# 2. ライブラリをインポートする\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#日本語対応\n",
    "sv.config_parser.read_string(\"[General]\\nuse_cjk_font=1\")\n",
    "\n",
    "# データセットの読み込み\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# 3. データを訓練データとテストデータに分割する\n",
    "train_data, test_data = train_test_split(df_rev, test_size=0.2)\n",
    "\n",
    "# 4. Sweetvizのanalyze関数とcompare関数を使用して、訓練データとテストデータを分析・比較する\n",
    "report = sv.compare([train_data, \"Training Data\"], [test_data, \"Test Data\"])\n",
    "\n",
    "# 5. レポートを表示する\n",
    "report.show_html(f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "じゅうかいき分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_rev.drop(['Url',\"名称\",\"地域\",\"Category\"], axis=1)\n",
    "x = df_rev.drop(['Url',\"名称\",\"地域\"], axis=1)\n",
    "dummies = pd.get_dummies(x, drop_first=True)\n",
    "# dummies\n",
    "print(np.isnan(dummies).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "欠損値を補完する:  ここでdfの名前変更しているので、実施要。\n",
    "一般的なアプローチとして、平均値や中央値、最頻値などを用いて欠損値を補完する方法があります。また、より高度な方法としては、他の特徴量を使用して欠損値を予測するモデルを訓練する方法もあります。\n",
    "ここでは、SimpleImputer を使用して欠損値を補完する方法を示します。　　---これをするとnumpyになってしまう。\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dummies_imputed = imputer.fit_transform(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎥 「特徴量の重要度を視覚化！ - ランダムフォレストを使ってみた！」\n",
    "\n",
    "🎉 こんにちは、皆さん！今日はあるグラフについてお話ししたいと思います。このグラフ、何か特別なものかというと、実は、車の価格を予測するのに役立つ情報、つまり、どの情報が一番大切なのかを見るためのものなんです。\n",
    "\n",
    "✨ ポイント1：バーの長さが大事！\n",
    "\n",
    "このグラフの中で、一番気をつけてほしいのは各バーの長さです。長いバーほど、その情報が価格の予測に重要ということを示しています。簡単に言うと、長いバーを持つ項目は、車の価格を知るためのキーポイントなんです！\n",
    "🚗 ポイント2：全部の情報が大事ではない！\n",
    "\n",
    "グラフを見て分かる通り、すべての情報が同じくらい大切なわけではありません。ある情報は非常に価値がある一方で、他の情報はそれほど影響を持たないことがあります。これは、例えば、ある車の機能は価格に大きく影響するけど、別の小さな機能はそんなに影響しない、ということを示しています。\n",
    "🔍 ポイント3：明確なランキングができる！\n",
    "\n",
    "このグラフを使えば、どの情報が最も価値があるのか、次にどれが大切なのか、というように、情報の重要度をはっきりとランキングすることができます。これは、車を買う時や売る時に、どんなポイントに注目すればいいのかを知る手助けになりますね！\n",
    "以上、ランダムフォレストを使って見えてきた車の価格予測に役立つ情報の重要度についてのポイントをお伝えしました！興味が湧いた方は、是非、この技術を使ってみてくださいね！🌟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode_dataframe(dataframe):\n",
    "    result_df = dataframe.copy()\n",
    "    le_dict = {}  # 各列のLabelEncoderを保存するための辞書\n",
    "\n",
    "    # データフレーム内のオブジェクト型の列を検索\n",
    "    for column in dataframe.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        result_df[column] = le.fit_transform(dataframe[column])\n",
    "        le_dict[column] = le  # 逆変換のためにLabelEncoderを保存\n",
    "        \n",
    "    return result_df, le_dict\n",
    "\n",
    "encoded_df, le_dict = label_encode_dataframe(dummies)\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ペアプロットの作成\n",
    "'''\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# 日本語フォントの設定\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# 必要な列だけを取得\n",
    "subset_df = df_rev.copy()\n",
    "\n",
    "# ペアプロットを描画\n",
    "g = sns.pairplot(data=subset_df, hue='グレード名')\n",
    "\n",
    "# 各プロットに相関係数を追加\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    # 数値型の列のみを考慮\n",
    "    if (np.issubdtype(subset_df.iloc[:, i].dtype, np.number) and\n",
    "            np.issubdtype(subset_df.iloc[:, j].dtype, np.number)):\n",
    "        g.axes[i, j].annotate(\n",
    "            f\"corr={subset_df.iloc[:, i].corr(subset_df.iloc[:, j]):.2f}\",\n",
    "            (0.5, 0.9), xycoords='axes fraction',\n",
    "            ha='center', va='center', fontsize=10\n",
    "        )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KFold または StratifiedKFold を使用してデータを複数のトレイン/テストセットに分割します。\n",
    "各分割でモデルをトレインし、特徴量の重要度を取得します。\n",
    "すべての分割で取得した特徴量の重要度の平均を計算します。\n",
    "平均の特徴量の重要度をプロットします。\n",
    "以下は、クロスバリデーションを追加したバージョンです。\n",
    "'''\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# データを特徴量とターゲットに分割\n",
    "X = encoded_df.drop('価格', axis=1)\n",
    "Y = encoded_df['価格']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# クロスバリデーションの設定\n",
    "'''\n",
    "k-Fold Cross Validationは手元のデータをk個のグループに分割して,k個のうちひとつのグループをテストデータとして\n",
    "残りのデータを学習データとします．それを全てのグループがテストデータになるようk回繰り返します．\n",
    "'''\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 各分割での特徴量の重要度を格納するリスト\n",
    "feature_importances_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_imputed):\n",
    "    X_train, X_test = X_imputed[train_index], X_imputed[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    # ランダムフォレストモデルの作成\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # モデルをデータに適合\n",
    "    rf.fit(X_train, Y_train)\n",
    "\n",
    "    # 特徴量の重要度を取得\n",
    "    feature_importances_list.append(rf.feature_importances_)\n",
    "\n",
    "# すべての分割での特徴量の重要度の平均を取得\n",
    "average_feature_importances = np.mean(feature_importances_list, axis=0)\n",
    "\n",
    "# 重要度の降順で特徴量をソート\n",
    "sorted_idx = average_feature_importances.argsort()\n",
    "\n",
    "# 平均の特徴量の重要度をプロット\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(X.columns[sorted_idx], average_feature_importances[sorted_idx])\n",
    "plt.xlabel(\"Average Feature Importance across CV\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"CrossValidationでのランダムフォレストによる平均特徴量の重要度\")\n",
    "\n",
    "# 画像として保存\n",
    "plt.savefig(\"feature_importances_amg_Ctouring.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #標準化の処理\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # モデル構築のためのインポート\n",
    "# from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "# # データ分割のためのインポート\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # 特徴量と目的変数を分離\n",
    "# X = encoded_df.drop(columns='価格')\n",
    "# y = encoded_df['価格']\n",
    "# # 標準化\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "# # 訓練データとテストデータに分割\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# # 各クラスを初期化してdict型変数のmodelsに格納\n",
    "# models = {\n",
    "#     'linear': LinearRegression(),\n",
    "#     'ridge': Ridge(random_state=0),\n",
    "#     'lasso': Lasso(random_state=0)}\n",
    "\n",
    "# # 正解率を格納するdict型変数を初期化\n",
    "# scores = {}\n",
    "\n",
    "# # 各モデルを順次生成し、正解率を算出して格納\n",
    "# for model_name, model in models.items():\n",
    "#     # モデル生成\n",
    "#     model.fit(X_train, Y_train)\n",
    "#     # 訓練データの正解率\n",
    "#     scores[(model_name, 'train')] = model.score(X_train, Y_train)\n",
    "#     # テストデータの正解率\n",
    "#     scores[(model_name, 'test')] = model.score(X_test, Y_test)\n",
    "\n",
    "# # dict型をpandasの1次元リストに変換\n",
    "# print(pd.Series(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重回帰予測式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# 1. データの準備\n",
    "X = encoded_df.drop(columns='価格')\n",
    "y = encoded_df['価格']\n",
    "\n",
    "# 2. データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. 重回帰モデルの訓練\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 訓練データでの予測\n",
    "train_predicted = reg.predict(X_train)\n",
    "# テストデータでの予測\n",
    "test_predicted = reg.predict(X_test)\n",
    "\n",
    "# R^2 スコアの計算\n",
    "train_score = reg.score(X_train, y_train)\n",
    "test_score = reg.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training R^2 score: {train_score:.2f}\")\n",
    "print(f\"Testing R^2 score: {test_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 4. 欠損値の確認・補完\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# 5. 予測式の表示\n",
    "print(\"Coefficients:\", reg.coef_)\n",
    "print(\"Intercept:\", reg.intercept_)\n",
    "print(\"予測式: \\n\")\n",
    "print(f\"価格 = {reg.intercept_:.4f}\")\n",
    "for coef, feature in zip(reg.coef_, X.columns):\n",
    "    print(f\" + ({coef:.4f} * {feature})\")\n",
    "\n",
    "# 6. 予測値の計算\n",
    "predicted = reg.predict(X)\n",
    "\n",
    "# 7. 予測値をdummiesに追加\n",
    "encoded_df['predicted_price'] = predicted\n",
    "print(encoded_df[['価格', 'predicted_price']])\n",
    "\n",
    "# 8. 予測値と実際の価格の差分を計算\n",
    "encoded_df['difference'] = encoded_df['価格'] - encoded_df['predicted_price']\n",
    "print(encoded_df[['価格', 'predicted_price', 'difference']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"価格 = {reg.intercept_:.4f}\")\n",
    "for coef, feature in zip(reg.coef_, X.columns):\n",
    "    print(f\" + ({coef:.4f} * {feature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n",
    "df1['difference'] = encoded_df['difference']\n",
    "df1['predicted_price']=encoded_df['predicted_price']\n",
    "\n",
    "# df1の特定の列を取得\n",
    "export_data = df1[['名称','年式','グレード名' ,'年' ,'色' ,'Class_Category', 'Url', 'Mission','修復歴'\n",
    "                   , 'difference','新車価格', '価格', '総額','predicted_price','走行距離']]\n",
    "# nanを含む行を削除\n",
    "export_data = export_data.dropna()\n",
    "\n",
    "\n",
    "# CSVにエクスポート\n",
    "filename = f\"{today_date}_{model_name}_withexpectedPrice.csv\"\n",
    "# df_rev.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "export_data.to_csv(f\"{filename}\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bokeh　による可視化。　散布図を作成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SUb1 更新処理\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
